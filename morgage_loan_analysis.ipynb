{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlewa\\AppData\\Local\\Temp\\ipykernel_24864\\2772946384.py:14: DtypeWarning: Columns (34,36,38,44,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hmda2007_0 = pd.read_csv('raw_data\\hmda_2007_0.csv')\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Load data from cleaned csv files\n",
    "# hmda = pd.read_csv('cleaned.csv')\n",
    "\n",
    "# Copy relative path for each file name \n",
    "# Load in chucks to avoid memory errors\n",
    "\n",
    "# Jack's data\n",
    "# hmda2007 = pd.read_csv('raw_data\\hmda_2007.csv')\n",
    "# hmda2008 = pd.read_csv('raw_data\\hmda_2008.csv')\n",
    "# hmda2009 = pd.read_csv('raw_data\\hmda_2009.csv')\n",
    "# hmda2010 = pd.read_csv('raw_data\\hmda_2010.csv')\n",
    "# hmda2011 = pd.read_csv('raw_data\\hmda_2011.csv')\n",
    "# hmda2012 = pd.read_csv('raw_data\\hmda_2012.csv')\n",
    "\n",
    "# Hridansh's data\n",
    "# hmda2013 = pd.read_csv('raw_data\\hmda_2013.csv')\n",
    "# hmda2014 = pd.read_csv('raw_data\\hmda_2014.csv')\n",
    "# hmda2015 = pd.read_csv('raw_data\\hmda_2015.csv')\n",
    "# hmda2016 = pd.read_csv('raw_data\\hmda_2016.csv')\n",
    "# hmda2017 = pd.read_csv('raw_data\\hmda_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def clean_data(data):\n",
    "    # Columns to keep\n",
    "    keep = ['as_of_year', 'action_taken', 'loan_type', 'loan_purpose', 'loan_amount_000s', 'msamd', 'state_code', 'county_code', 'applicant_ethnicity', 'co_applicant_ethnicity',\n",
    "            'applicant_race_1', 'co_applicant_race_1', 'applicant_sex', 'co_applicant_sex', 'applicant_income_000s', 'purchaser_type', 'rate_spread', 'hoepa_status',\n",
    "            'population', 'minority_population', 'hud_median_family_income', 'tract_to_msamd_income', 'number_of_owner_occupied_units', 'number_of_1_to_4_family_units']\n",
    "\n",
    "    # Remove columns that are not in the keep list\n",
    "    data = data[keep]\n",
    "\n",
    "    # Remove irrelevant values from columns\n",
    "    # applicant_ethnicity: 3, 4\n",
    "    data = data[~data['applicant_ethnicity'].isin([3, 4])]\n",
    "    # co_applicant_ethnicity: 3, 4\n",
    "    data = data[~data['co_applicant_ethnicity'].isin([3, 4])]\n",
    "    # applicant_race_1: 6, 7\n",
    "    data = data[~data['applicant_race_1'].isin([6, 7])]\n",
    "    # co_applicant_race_1: 6, 7\n",
    "    data = data[~data['co_applicant_race_1'].isin([6, 7])]\n",
    "    # applicant_sex: 3\n",
    "    data = data[~data['applicant_sex'].isin([3])]\n",
    "    # co_applicant_sex: 3\n",
    "    data = data[~data['co_applicant_sex'].isin([3])]\n",
    "    # hoepa_status: 3, 4\n",
    "    data = data[~data['hoepa_status'].isin([3, 4])]\n",
    "    # action_taken: 4, 5, 6\n",
    "    data = data[~data['action_taken'].isin([4, 5, 6])]\n",
    "\n",
    "\n",
    "    # Recode values for interpretation purposes\n",
    "    # Set 'No co applicant' to 0 for co_applicant_ethnicity, co_applicant_race_1, and co_applicant_sex\n",
    "    co_apps1 = {'co_applicant_ethnicity': 5, 'co_applicant_sex': 5}\n",
    "    for column, value in co_apps1.items():\n",
    "        data.loc[data[column] == value, column] = 0\n",
    "\n",
    "    co_apps2 = {'co_applicant_race_1': 8}\n",
    "    for column, value in co_apps2.items():\n",
    "        data.loc[data[column] == value, column] = 0\n",
    "\n",
    "    # Set 'Not applicable' to 3 for applicant_sex and co_applicant_sex\n",
    "    app_sex = {'applicant_sex': 4, 'co_applicant_sex': 4}\n",
    "    for column, value in app_sex.items():\n",
    "        data.loc[data[column] == value, column] = 3\n",
    "\n",
    "    # Set action taken to 1 for approved and 0 for denied\n",
    "    for value in [2, 8]:\n",
    "        data.loc[data['action_taken'] == value, 'action_taken'] = 1\n",
    "\n",
    "    for value in [3, 7]:\n",
    "        data.loc[data['action_taken'] == value, 'action_taken'] = 0\n",
    "    \n",
    "    # Set rate_spread nan values to -1 for denied loans they removed during nan removal\n",
    "    data.loc[data['action_taken'] == 0, 'rate_spread'] = -1\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Clean and save cleaned data to csv file\n",
    "\n",
    "# Jack's data\n",
    "# cleaned_2007 = clean_data(hmda2007)\n",
    "# cleaned_2008 = clean_data(hmda2008)\n",
    "# cleaned_2009 = clean_data(hmda2009)\n",
    "# cleaned_2010 = clean_data(hmda2010)\n",
    "# cleaned_2011 = clean_data(hmda2011)\n",
    "# cleaned_2012 = clean_data(hmda2012)\n",
    "\n",
    "# Hridansh's data\n",
    "# cleaned_2013 = clean_data(hmda2013)\n",
    "# cleaned_2014 = clean_data(hmda2014)\n",
    "# cleaned_2015 = clean_data(hmda2015)\n",
    "# cleaned_2016 = clean_data(hmda2016)\n",
    "# cleaned_2017 = clean_data(hmda2017)\n",
    "\n",
    "# Merge dataframes and save cleaned data to csv\n",
    "# Jack's data\n",
    "# cleaned_07_08 = pd.concat([cleaned_2007, cleaned_2008])\n",
    "# cleaned_07_08.to_csv('hmda_cleaned07_08.csv', index=False)\n",
    "# cleaned_09_10 = pd.concat([cleaned_2009, cleaned_2010])\n",
    "# cleaned_09_10.to_csv('hmda_cleaned09_10.csv', index=False)\n",
    "# cleaned_11_12 = pd.concat([cleaned_2011, cleaned_2012])\n",
    "# cleaned_11_12.to_csv('hmda_cleaned11_12.csv', index=False)\n",
    "\n",
    "# cleaned_07_08 = pd.read_csv('hmda_cleaned07_08.csv')\n",
    "# cleaned_09_10 = pd.read_csv('hmda_cleaned09_10.csv')\n",
    "# cleaned_11_12 = pd.read_csv('hmda_cleaned11_12.csv')\n",
    "# cleaned_07_12 = pd.concat([cleaned_07_08, cleaned_09_10, cleaned_11_12])\n",
    "# cleaned_07_12.to_csv('hmda_cleaned07_12.csv', index=False)\n",
    "\n",
    "# Hridansh's data\n",
    "# cleaned_13_14 = pd.concat([cleaned_2013, cleaned_2014])\n",
    "# cleaned_13_14.to_csv('hmda_cleaned13_14.csv', index=False)\n",
    "\n",
    "# cleaned_15_16 = pd.concat([cleaned_2015, cleaned_2016])\n",
    "# cleaned_15_16.to_csv('hmda_cleaned15_16.csv', index=False)\n",
    "\n",
    "# cleaned_17 = cleaned_2017\n",
    "# cleaned_17.to_csv('hmda_cleaned17.csv', index=False)\n",
    "\n",
    "# cleaned_13_14 = pd.read_csv('hmda_cleaned13_14.csv')\n",
    "# cleaned_15_16 = pd.read_csv('hmda_cleaned15_16.csv')\n",
    "# cleaned_17 = pd.read_csv('hmda_cleaned17.csv')\n",
    "# cleaned_13_17 = pd.concat([cleaned_13_14, cleaned_15_16, cleaned_17])\n",
    "\n",
    "# Once all data is cleaned and compiled by Hridansh and Jack, compile all data into one file\n",
    "# Complied cleaned data into one file\n",
    "# cleaned_07_12 = pd.read_csv('hmda_cleaned07_12.csv')\n",
    "# cleaned_13_17 = pd.read_csv('hmda_cleaned13_17.csv')\n",
    "# cleaned = pd.concat([cleaned_07_12, cleaned_13_17])\n",
    "\n",
    "\n",
    "# Save cleaned data to csv\n",
    "# cleaned.to_csv('cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purchaser_type = 0 and rate_spead = -1 will both be perfectly correlated with 'loan denied', need to address before traing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
